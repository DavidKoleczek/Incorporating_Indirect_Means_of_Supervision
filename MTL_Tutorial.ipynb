{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Indirect Means of Supervision in Practice\n",
    "#### Supervised learning requires labeled training data, a learnable model, and hardware. Thanks to open source implementations, we have high-performance algorithms available that are becoming ever easier to use. Thanks to the proliferation of cloud technology, we have as much compute available as our finances allow. But the bottleneck in many cases ends up becoming the amount and quality of training data that we have, especially if exceptional accuracy is needed (like in many medical applications). This motivates the problem of **finding non-traditional means of incorporating domain knowledge into our models.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this jupyter notebook we explore incorporating indirect means of supervision by tackling a challenging supervision problem which requires such methods to achieve quality results. We focus on a classic problem in natural language processing (NLP), sentiment analysis. We start from the basics by solving it using standard modeling techniques and show how we reach a limit in possible performance. We then sequentially add additional supervision **signals** and show how each can improve performance in this sentiment analysis task. We hope that this tutorial will help to bridge the gap between the recent advancements in this area to implementing them in practice.\n",
    "\n",
    "##### This work is heavily inspired by research born out of the Standard AI Lab and the creators of the open-source library Snorkel. Particular inspiration was taken from their writeup, [Massive Multi-Task Learning with Snorkel MeTaL: Bringing More Supervision to Bear](https://dawn.cs.stanford.edu/2019/03/22/glue/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem and Dataset \n",
    "\n",
    "### Sentiment analysis using the [Financial Phrase Bank](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip?origin=publication_list)  dataset\n",
    "A collection of ∼5000 financial/economic news texts. Annotated by humnas that were  screened to ensure that they have sufficient business knowledge and educational background. Each sentence in the dataset is labeled as positive, negative, or neutral.\n",
    "\n",
    "### Why Financial Phrase Bank? \n",
    "- Finacial data is often proprietary and scarce. \n",
    "    - Any improvement we can get from publicly available methods, domain knowledge, or data we thought previously could not be applied is immensely valuable.\n",
    "- Due to the heavy dependence on semantic meaning in determining sentiment, it is a challenging problem where using only traditional supervision might not provide satisfactory results.\n",
    "- Either of previous two points are common with many other problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "from mtl_helpers import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "data = read_finphrase('data/Sentences_66Agree.txt')\n",
    "data = pd.DataFrame(data, columns=['sentence', 'label'])\n",
    "data = data.dropna()\n",
    "data = data.sample(frac=1, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>The Bristol Port Company has sealed a one million pound contract with Cooper Specialised Handling to supply it with four 45-tonne , customised reach stackers from Konecranes .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>ADPnews - Sep 28 , 2009 - Finnish silicon wafers maker Okmetic Oyj HEL : OKM1V said it will reduce the number of its clerical workers by 22 worldwide as a result of personnel negotiations completed today .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>Nordstjernan has used its option to buy another 22.4 % stake of Salcomp 's shares and votes .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>The office space will rise above the remodeled Cannon Street underground station .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Operating result showed a loss of EUR 2.9 mn , while a year before , it showed a profit of EUR 0.6 mn .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>Ruukki 's delivery volumes and selling prices showed favourable development and the company 's comparable net sales grew by 50 % year-on-year to EUR647m , CEO Sakari Tamminen said .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           sentence  \\\n",
       "1860                                The Bristol Port Company has sealed a one million pound contract with Cooper Specialised Handling to supply it with four 45-tonne , customised reach stackers from Konecranes .   \n",
       "1396  ADPnews - Sep 28 , 2009 - Finnish silicon wafers maker Okmetic Oyj HEL : OKM1V said it will reduce the number of its clerical workers by 22 worldwide as a result of personnel negotiations completed today .   \n",
       "2137                                                                                                                  Nordstjernan has used its option to buy another 22.4 % stake of Salcomp 's shares and votes .   \n",
       "1808                                                                                                                             The office space will rise above the remodeled Cannon Street underground station .   \n",
       "123                                                                                                         Operating result showed a loss of EUR 2.9 mn , while a year before , it showed a profit of EUR 0.6 mn .   \n",
       "3091                          Ruukki 's delivery volumes and selling prices showed favourable development and the company 's comparable net sales grew by 50 % year-on-year to EUR647m , CEO Sakari Tamminen said .   \n",
       "\n",
       "      label  \n",
       "1860      1  \n",
       "1396      0  \n",
       "2137      2  \n",
       "1808      2  \n",
       "123       0  \n",
       "3091      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 is \"positive\", 0 is \"negative\", 2 is \"neutral\"\n",
    "data.sample(6, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and split data\n",
    "# NOTE: Temporarily simplify the problem to binary classification, i.e. just the negative and positive samples\n",
    "data = data[data.label != 2]\n",
    "train_split_idx = 1300\n",
    "\n",
    "x_train = data[0:train_split_idx]['sentence']\n",
    "y_train = data[0:train_split_idx]['label']\n",
    "\n",
    "x_val = data[train_split_idx:]['sentence']\n",
    "y_val = data[train_split_idx:]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 1: Traditional Supervision\n",
    "### We will train a few standard neural network architectures to get an idea of how well we can do with only this small dataset.\n",
    "- A basic multilayer perceptron with dropout and the data encoded using tf-idf gets us an accuracy of **~85%**. \n",
    "- Embedding the data using GloVe and using a couple Bidirectional LSTM layers performs much worse with an accuracy of **~77%**. \n",
    "    - Perhaps this hints that more sophisticated architectures will not give us an increase in performance given our small amount of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6362 - accuracy: 0.6862 - val_loss: 0.5528 - val_accuracy: 0.7173\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.4799 - accuracy: 0.7038 - val_loss: 0.4507 - val_accuracy: 0.7801\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 0s 4ms/step - loss: 0.2451 - accuracy: 0.9238 - val_loss: 0.3556 - val_accuracy: 0.8508\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9908 - val_loss: 0.3567 - val_accuracy: 0.8508\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.9969 - val_loss: 0.3930 - val_accuracy: 0.8482\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9985 - val_loss: 0.4135 - val_accuracy: 0.8560\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 0.9977 - val_loss: 0.4388 - val_accuracy: 0.8560\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4702 - val_accuracy: 0.8534\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.4854 - val_accuracy: 0.8560\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4961 - val_accuracy: 0.8560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22536ef87c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we train a MLP with our text encoded using  term frequency–inverse document frequency\n",
    "\n",
    "# Parameters for tf-idf\n",
    "TOP_K = 20000\n",
    "tfidf_args = {\n",
    "    'ngram_range': (1, 2),\n",
    "    'dtype': 'int32',\n",
    "    'strip_accents': 'unicode',\n",
    "    'decode_error': 'replace',\n",
    "    'stop_words': ['a', 'an', 'the', 'i'],\n",
    "    'analyzer': 'word',  # Split text into word tokens.\n",
    "    'min_df': 1, # Minimum document/corpus frequency below which a token will be discarded.\n",
    "    'max_df' : 0.33,\n",
    "    'dtype': np.float64,\n",
    "}\n",
    "\n",
    "tfvect = TfidfVectorizer(**tfidf_args)\n",
    "x_train_tf = tfvect.fit_transform(x_train)\n",
    "x_val_tf = tfvect.transform(x_val)\n",
    "\n",
    "# We also select the top-k features by using the ANOVA F-value. \n",
    "selector = SelectKBest(f_classif, k=min(TOP_K, x_train_tf.shape[1]))\n",
    "selector.fit(x_train_tf, y_train)\n",
    "x_train_tf = selector.transform(x_train_tf).toarray()\n",
    "x_val_tf = selector.transform(x_val_tf).toarray()\n",
    "\n",
    "# Create the MLP Keras model.\n",
    "# Note that we use \n",
    "inputs = keras.Input(shape = (x_val_tf.shape[1],))\n",
    "x = layers.Dense(64, activation = 'relu')(inputs)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(64, activation = 'relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "outputs = layers.Dense(2, activation ='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001)\n",
    "model.compile(optimizer = optimizer, loss ='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_tf, y_train, batch_size = 32, epochs = 10, validation_data=(x_val_tf, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 GloVe word vectors.\n",
      "Converted 3914 words (790 misses)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/15\n",
      "41/41 [==============================] - 15s 366ms/step - loss: 0.6132 - accuracy: 0.6885 - val_loss: 0.5998 - val_accuracy: 0.7199\n",
      "Epoch 2/15\n",
      "41/41 [==============================] - 15s 354ms/step - loss: 0.5834 - accuracy: 0.7000 - val_loss: 0.5931 - val_accuracy: 0.6780\n",
      "Epoch 3/15\n",
      "41/41 [==============================] - 15s 357ms/step - loss: 0.5441 - accuracy: 0.7254 - val_loss: 0.5702 - val_accuracy: 0.7277\n",
      "Epoch 4/15\n",
      "41/41 [==============================] - 15s 371ms/step - loss: 0.5196 - accuracy: 0.7454 - val_loss: 0.5635 - val_accuracy: 0.7304\n",
      "Epoch 5/15\n",
      "41/41 [==============================] - 15s 357ms/step - loss: 0.4740 - accuracy: 0.7723 - val_loss: 0.5487 - val_accuracy: 0.7304\n",
      "Epoch 6/15\n",
      "41/41 [==============================] - 16s 383ms/step - loss: 0.4308 - accuracy: 0.8162 - val_loss: 0.5427 - val_accuracy: 0.7539\n",
      "Epoch 7/15\n",
      "41/41 [==============================] - 15s 375ms/step - loss: 0.3737 - accuracy: 0.8454 - val_loss: 0.6396 - val_accuracy: 0.6649\n",
      "Epoch 8/15\n",
      "41/41 [==============================] - 15s 375ms/step - loss: 0.3648 - accuracy: 0.8369 - val_loss: 0.5381 - val_accuracy: 0.7696\n",
      "Epoch 9/15\n",
      "41/41 [==============================] - 15s 374ms/step - loss: 0.2729 - accuracy: 0.8908 - val_loss: 0.5408 - val_accuracy: 0.7696\n",
      "Epoch 10/15\n",
      "41/41 [==============================] - 15s 377ms/step - loss: 0.2247 - accuracy: 0.9115 - val_loss: 0.6349 - val_accuracy: 0.7382\n",
      "Epoch 11/15\n",
      "41/41 [==============================] - 15s 374ms/step - loss: 0.1829 - accuracy: 0.9269 - val_loss: 0.7592 - val_accuracy: 0.6990\n",
      "Epoch 12/15\n",
      "41/41 [==============================] - 15s 374ms/step - loss: 0.1320 - accuracy: 0.9569 - val_loss: 0.7453 - val_accuracy: 0.7749\n",
      "Epoch 13/15\n",
      "41/41 [==============================] - 16s 382ms/step - loss: 0.1179 - accuracy: 0.9546 - val_loss: 0.8839 - val_accuracy: 0.7330\n",
      "Epoch 14/15\n",
      "41/41 [==============================] - 16s 381ms/step - loss: 0.1154 - accuracy: 0.9531 - val_loss: 0.6973 - val_accuracy: 0.7592\n",
      "Epoch 15/15\n",
      "41/41 [==============================] - 17s 404ms/step - loss: 0.1167 - accuracy: 0.9454 - val_loss: 0.6759 - val_accuracy: 0.7775\n"
     ]
    }
   ],
   "source": [
    "# Next will will try a more sophisticated architecture, Birectional LSTM. Also we embed our data using pretrained GloVe word vectors. \n",
    "\n",
    "# Code largely courtesy of:  https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "vectorizer, embeddings = create_vectorizer_and_embeddings(x_train)\n",
    "\n",
    "# Convert our data to vectorized form, see definition of create_vectorizer_and_embeddings for more information\n",
    "x_train_vectorized = vectorizer(np.array([[s] for s in x_train])).numpy()\n",
    "x_val_vectorized = vectorizer(np.array([[s] for s in x_val])).numpy()\n",
    "\n",
    "# Make sure our data is 1D NumPy arrays\n",
    "y_train_vectorized = np.array(y_train)\n",
    "y_val_vectorized = np.array(y_val)\n",
    "\n",
    "# Create embedding layer for GloVe vectors.\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim = len(vectorizer.get_vocabulary()) + 2,\n",
    "    output_dim = 100,\n",
    "    embeddings_initializer=keras.initializers.Constant(embeddings),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "# Define the BiLSTM network\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Bidirectional(layers.LSTM(50, return_sequences=True, recurrent_dropout = 0.2))(embedded_sequences)\n",
    "x = layers.Bidirectional(layers.LSTM(50, return_sequences=False, recurrent_dropout = 0.2))(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "preds = layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "with tf.device('/cpu:0'): # Running on GPU was a lot slower...\n",
    "    model.fit(x_train_vectorized, y_train_vectorized, batch_size=32, epochs=15, validation_data=(x_val_vectorized, y_val_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 2: Transfer Learning - Applying knowledge gained on one problem to another\n",
    "### [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) is a massive pretrained language model. It is a contextual model that generates a representation of each word based on the other words in the sentence. It has various forms, but the one we will use has 110 million parameters and was trained on the BooksCorpus which has 800 million words. As we will show, BERT  can be fine-tuned with just one additional output layer to achieve state-of-the-art performance for a wide range of tasks, without the need for task-specific architectural modifications. Training times are also incredibly manageable given the size of the network. One iteration (which is almost all you need) on the PhraseBank dataset takes under 20 seconds with a NVIDIA 2070 Super, a midrange consumer-grade GPU. \n",
    "\n",
    "Using only BERT, we are able to achieve **~92%** validation accuracy on this problem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Tokenize our data using BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_bert = convert_data(bert_tokenizer, x_train, y_train).batch(BATCH_SIZE)\n",
    "val_bert = convert_data(bert_tokenizer, x_val, y_val).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier', 'dropout_40']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "82/82 [==============================] - 20s 243ms/step - loss: 0.3483 - accuracy: 0.8454 - val_loss: 0.2279 - val_accuracy: 0.8953\n",
      "Epoch 2/2\n",
      "82/82 [==============================] - 18s 217ms/step - loss: 0.0967 - accuracy: 0.9608 - val_loss: 0.2297 - val_accuracy: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2270e33f048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Bert Model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.fit(train_bert, batch_size = BATCH_SIZE, epochs=2, validation_data=val_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 3: External Features\n",
    "\n",
    "Many methods for sentiment analysis already exist. These methods are generic and are generally rule-based. \n",
    "Some examples are: \n",
    "- [VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text](http://eegilbert.org/papers/icwsm14.vader.hutto.pdf). An implementation is available in [nltk](https://www.nltk.org/). \n",
    "- [TextBlob's sentiment analysis](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis)\n",
    "- Other methods available in [nltk's sentiment analysis](http://www.nltk.org/api/nltk.sentiment.html#nltk-sentiment-package) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 4: Multitask Learning (MTL)\n",
    "### MTL is whenever we use a shared representation, where what is learned for each individual task helps the other tasks be learned better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 5: Dataset Slicing \n",
    "### As we examine where our model makes errors, we might recognize **slices** of data (subsets of the data with some property in common) where our model consistently underperforms. If we can identify heuristics where our accuracy is lower, we can leverage that to tell the model where it needs to pay more attention.\n",
    "\n",
    "For example, given a slice of underperforming data, we can use the MTL paradigm where we add another task that is only that slice of data. Thus, we can explictly train the model on those underperforming examples, with the hopes that it will learn them better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 6: Data Augmentation\n",
    "### This is a broad technique that encompasses ways to increase your training by apply transformations to it. The most well-known applications are in computer vision, such as rotating existing images in your dataset. However, NLP is another field where data augmentation is becoming more important and common. \n",
    "\n",
    "[Visual Survey of Data Augmentation in NLP](https://amitness.com/2020/05/data-augmentation-for-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pronoun replacement via Named Entity Recognition (NER)\n",
    "Looking at the data instances, we see that there are a lot of specific company names. But given an individual text, how much does what the actual company name is matter compared to the context of how it's being used. We could detect the unique company names in a sentence using NER, and impute each company name by instance with Company A, Company B, ... for each unique company in the text. \n",
    "This would serve to simplify the data which could improve generalization, but also it injects the signal of which tokens are companies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-tuning GPT-3 to generate synthetic instances\n",
    "GPT-3 is a pretrained language model (like BERT) that is tuned to producing human-like text. The application here is to fine-tune GPT-3 on the positive samples, then use it to generate new, synthetic, positive instances. The same could be done for any class, and could be thought of as an oversampling method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 7: Ensembling\n",
    "### While solving a problem, we may be presented with a choice. Usually this comes with some tradeoff and we have to make the decision about which choice will lead to the best result. But in making this choice sometimes we lose out on some signal from the option we did not take. With ensembling we do not make the choice, instead we somehow combine the results of each choice together. \n",
    "\n",
    "Previously when we used BERT we had a choice between using the cased and uncased versions of it. We originally chose the cased version because we believe that our problem relies heavily on recognizing pronouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal 8+?: Active Area of Research\n",
    "#### New research is published frequently\n",
    "- [AutoSimulate: (Quickly) Learning Synthetic Data Generation](https://arxiv.org/pdf/2008.08424.pdf) - University of Oxford and Microsoft Research\n",
    "    - Paper released August 16, 2020 about an efficient alternative for optimal synthetic data generation.\n",
    "- [RandAugment: Practical automated data augmentation with a reduced search space](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w40/Cubuk_Randaugment_Practical_Automated_Data_Augmentation_With_a_Reduced_Search_Space_CVPRW_2020_paper.pdf) - Google Brain\n",
    "    - Methods for automatically finding the best data augmention strategies for vision tasks.\n",
    "    \n",
    "#### Some of the most exciting technology being worked on right now relies heavily on these concepts. \n",
    "##### Autonomous driving \n",
    "- [Tesla Autopilot and Multi-Task Learning for Perception and Prediction](https://www.youtube.com/watch?v=IHH47nZ7FZU&t=127s) - Andrej Karpathy, Director of AI at Tesla. \n",
    "    - Talk about how Tesla leverages MTL for getting performance needed for perception tasks.  \n",
    "- [Using automated data augmentation to advance our Waymo Driver](https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html) - Blog from Waymo\n",
    "    - Blog about how researchers at Waymo use data augmentation to improve perception tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
